---
title: "Understading_tensors"
author: "Z.CAI"
date: "2021年11月25日"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# understanding the tensor

```{r}

library(keras)

mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y

```

# the number of axes of the tensor 'train_images', its shape and its data type

```{r}
length(dim(train_images))
dim(train_images)
typeof(train_images)
```
# e.g. plot the fifth digit in this 3D tensor

```{r}
digit <- train_images[5,,] 
plot(as.raster(digit, max = 255))
```

# tensor slicing examples

```{r}
my_slice1 <- train_images[10:99,,]
my_slice1

my_slice2 <- train_images[, 15:28, 15:28]
```

# break examples of batches

```{r}
batch1 <- train_images[1:128,,]
batch2 <- train_images[129:256,,]
```

# naive relu operation

```{r}
naive_relu <- function(x) { 
  for (i in nrow(x))
    for (j in ncol(x)) 
      x[i, j] <- max(x[i, j], 0)
  x 
  }
```

# naive addtion operation 

```{r}
naive_add <- function(x, y) {
  for (i in nrow(x))
    for (j in ncol(x))
      x[i, j] = x[i, j] + y[i, j]
  x
}
```

# BLAS (Basic Linear Algebra Subprograms)

```{r}
# z <- x + y
# z <- pmax(z, 0)
```

# sweep() perform operation between higher-dimension tensors and lowers

sweep(x, 2, y, '+')

The second argument (here, 2) specifies the dimensions of x over which to sweep y. The last argument (here, +) is the operation to perform during the sweep, which should be a function of two arguments: x and an array of the same dimensions generated from y by aperm().

You can apply a sweep in any number of dimensions and can apply any function that implements a vectorized operation over two arrays. The following example sweeps a 2D tensor over the last two dimensions of a 4D tensor using the pmax() function:

```{r}
x <- array(round(runif(1000, 0, 9)), dim = c(64, 3, 32, 10))
y <- array(5, dim = c(32, 10))

z <- sweep(x, c(3, 4), y, pmax)
```

# tensor dot operation %*%

z <- x %*% y

the dot product of two vectors x and y
```{r}
naive_vector_dot <- function(x, y){
  z <- 0 
  for (i in 1:length(x))
       z <- z + x[[i]] * y[[i]]
  z
}

```

You’ll have noticed that the dot product between two vectors is a scalar and that only vectors with the same number of elements are compatible for a dot product

1 x and y are 1D tensors (vector)
```{r}
naive_matrix_vector_dot <- function(x, y){
  z <- rep(0, nrow(x))
  for (i in 1: nrow(x))
    for (j in 1:ncol(x))
      z[[i]] <- z[[i]] + x[[i, j]] * y[[j]]
  z
}
```

1 x is a 2D tensor (matrix). y is a 1D tensor (vector)
```{r}
naive_matrix_vector_dot <- function(x, y) { 
  z <- rep(0, nrow(x))
  for (i in 1:nrow(x))
    z[[i]] <- naive_vector_dot(x[i,], y) 
  z
}
```

Note that as soon as one of the two tensors has more than one dimension, %*%is no longer symmetric,
which is to say that x %*%y 
isn’t the same as y %*% x.
Of course, a dot product generalizes to tensors with an arbitrary number of axes. The most common applications may be the dot product between two matrices. You can take thedotproductoftwomatricesxandy(x %*% y)ifandonlyifncol(x) == nrow(y). The result is a matrix with shape (nrow(x), ncol(y)), where the coefficients are the vector products between the rows of x and the columns of y. 

1 x and y are 2D tensors (matrices)

```{r}
naive_matrix_dot <- function(x, y){
  z <- matrix(0, nrow = nrow(x), ncol = ncol(y))
  for (i in 1:nrow(x))
    for (j in 1:ncol(y)) { 
      row_x <- x[i,] 
      column_y <- y[,j]
      z[i, j] <- naive_vector_dot(row_x, column_y) 
    }
  z
}
```

(a, b, c, d) . (d) -> (a, b, c)
(a, b, c, d) . (d, e) -> (a, b, c, e)

# tensor reshaping 

```{r}
train_images <- array_reshape(train_images, c(60000, 28 * 28))
```

Note that we use the array_reshape() function rather than the dim<-() function to reshape the array. This is so that the data is reinterpreted using row-major semantics (as opposed to R’s default column-major semantics), which is in turn compatible with the way the numerical libraries called by Keras (NumPy, TensorFlow, and so on) interpret array dimensions. You should always use the array_reshape() function when reshaping R arrays that will be passed to Keras.
Reshaping a tensor means rearranging its rows and columns to match a target shape. Naturally, the reshaped tensor has the same total number of coefficients as the initial tensor

example:

```{r}
x <- matrix(c(0, 1,
              2, 3,
              4, 5),
            nrow = 3, ncol = 2, byrow = TRUE)
x
```
```{r}
x <- array_reshape(x, dim = c(6, 1))
x
```

```{r}
x <- array_reshape(x, dim = c(2, 3))
x
```

A special case of reshaping that’s commonly encountered is transposition. Transposing a matrix means exchanging its rows and its columns, so that x[i,] becomes x[, i]. The t() function can be used to transpose a matrix:

```{r}
x <- matrix(0, nrow = 300, ncol = 20)
dim(x)

x <- t(x)
dim(x)
```

In general, elementary geometric operations such as affine transformations, rotations, scaling, and so on can be expressed as tensor operations. For instance, a rotation of a 2D vector by an angle theta can be achieved via a dot product with a 2 × 2 matrix R = [u, v],whereuandvarebothvectorsoftheplane:u = [cos(theta), sin(theta)]andv = [-sin(theta), cos(theta)].


# The Engine of Neural Network: Gradient-Based Optimisation

```{r}
output = relu(dot(W, input) + b)
```

W and b are tensors that are attributes of the layer.
The weights or trainable parameters of the layer (the kernel and bias attributes, respectively). 
These weights contain the information learned by the network from exposure to training data.

Initially, these weight matrices are filled with small random values (a step called random initialization). Of course, there’s no reason to expect that relu(dot(W, input) 

+ b),when W and b are random, will yield anyuseful representations.The resulting representations are meaningless—but they’re a starting point. What comes next is to gradually adjust these weights, based on a feedback signal. This gradual adjustment, also called training, is basically the learning that machine learning is all about.

training loop (repeat these steps in a loop as long as necessary)
1. Draw a batch of training samples x and corresponding targets y.
2. Run the network on x (a step called the forward pass) to obtain predictions
y_pred.
3. Compute the loss of the network on the batch, a measure of the mismatch between
y_pred and y.
4. Updateallweightsofthenetworkinawaythatslightlyreducesthelossonthis
batch.

But such an approach would be horribly inefficient, because you’d need to compute two forward passes (which are expensive) for every individual coefficient (of which there are many, usually thousands and sometimes up to millions). A much better approach is to take advantage of the fact that all operations used in the network are differentiable, and compute the gradient of the loss with regard to the network’s coefficients. You can then move the coefficients in the opposite direction from the gradient, thus decreasing the loss.

# Derivative of a Tensor Operation: The Gradient

consider an input vector x,a matrix W, a target y, and a loss function loss. If the data inputs x and y are frozen, then this can be interpreted as a function mapping values of W to loss values

```{r}
y_pred = dot(W, x)
loss_value = loss(y_pred, y)

loss_value = f(w)
```

Hence the tensor gradient (f) (W) is the gradient of the function f(W) = loss_value in W

# Stochastic Gradient Descent

Given a differentiable function, it’s theoretically possible to find its minimum analytically: it’s known that a function’s minimum is a point where the derivative is 0, so all you have to do is find all the points where the derivative goes to 0 and check for which of these points the function has the lowest value. Applied to a neural network, that means finding analytically the combination of weight values that yields the smallest possible loss function. 

 This can be done by solving the equationgradient(f)(W) = 0forW.ThisisapolynomialequationofNvariables, where N is the number of coefficients in the network. 
 
1. Draw a batch of training samples x and corresponding targets y.
2. Run the network on x to obtain predictions y_pred.
3. Compute the loss of the network on the batch, a measure of the mismatch between
y_pred and y.
4. Computethegradientofthelosswithregardtothenetwork’sparameters(a
backward pass).
5. Move the parameters a little in the opposite direction from the gradient—for
example, W = W - (step * gradient)—thus reducing the loss on the batch a bit. 
 
Momentum implementation on parameter update

```{r}
past_velocity <- 0
momentum <- 0.1
while (loss > 0.1) {
  params <- get_current_parameters()
  w <- params$w
  loss <- params$loss
  gradient <- params$gradient
  
  velocity <- past_velocity * momentum + learning_rate * gradient
  w <- w + momentum * velocity - learning_rate * gradient
  past_velocity <- velocity
  update_parameter(w)
}
```

 